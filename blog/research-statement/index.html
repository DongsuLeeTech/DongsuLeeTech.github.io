<!DOCTYPE html>
<!-- saved from url=(0033)https://dongsulee.github.io/blog/research-statement/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async="" src="./rs_files/js"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());
        gtag('config', 'G-3LQTW4VHRT');
    </script>

    <title>Research Statement</title>

    <meta name="author" content="Dongsu Lee">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--<base target="_blank">--><base href="." target="_blank">

    <link rel="stylesheet" href="./rs_files/bootstrap.min.css">
    <link rel="stylesheet" href="./rs_files/font-awesome.min.css">
    <link rel="stylesheet" href="./rs_files/stylesheet.css">
    <link rel="stylesheet" href="./rs_files/app.css">
    <link rel="icon" type="image/png" sizes="32x32" href="https://dongsuleetech.github.io/images/favicon/Ds.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://dongsuleetech.github.io/images/favicon/Ds.png">

    <script src="./rs_files/jquery.min.js"></script>
    <script src="./rs_files/bootstrap.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            loader: {load: ['[tex]/color']},
            tex: {packages: {'[+]': ['color']}}
        };
    </script>

    <script src="./rs_files/app.js"></script>
<style type="text/css">.CtxtMenu_InfoClose {  top:.2em; right:.2em;}
.CtxtMenu_InfoContent {  overflow:auto; text-align:left; font-size:80%;  padding:.4em .6em; border:1px inset; margin:1em 0px;  max-height:20em; max-width:30em; background-color:#EEEEEE;  white-space:normal;}
.CtxtMenu_Info.CtxtMenu_MousePost {outline:none;}
.CtxtMenu_Info {  position:fixed; left:50%; width:auto; text-align:center;  border:3px outset; padding:1em 2em; background-color:#DDDDDD;  color:black;  cursor:default; font-family:message-box; font-size:120%;  font-style:normal; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 15px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius:15px;               /* Safari and Chrome */  -moz-border-radius:15px;                  /* Firefox */  -khtml-border-radius:15px;                /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */  filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color="gray", Positive="true"); /* IE */}
</style><style type="text/css">.CtxtMenu_MenuClose {  position:absolute;  cursor:pointer;  display:inline-block;  border:2px solid #AAA;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  font-family: "Courier New", Courier;  font-size:24px;  color:#F0F0F0}
.CtxtMenu_MenuClose span {  display:block; background-color:#AAA; border:1.5px solid;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  line-height:0;  padding:8px 0 6px     /* may need to be browser-specific */}
.CtxtMenu_MenuClose:hover {  color:white!important;  border:2px solid #CCC!important}
.CtxtMenu_MenuClose:hover span {  background-color:#CCC!important}
.CtxtMenu_MenuClose:hover:focus {  outline:none}
</style><style type="text/css">.CtxtMenu_Menu {  position:absolute;  background-color:white;  color:black;  width:auto; padding:5px 0px;  border:1px solid #CCCCCC; margin:0; cursor:default;  font: menu; text-align:left; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 5px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius: 5px;             /* Safari and Chrome */  -moz-border-radius: 5px;                /* Firefox */  -khtml-border-radius: 5px;              /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */}
.CtxtMenu_MenuItem {  padding: 1px 2em;  background:transparent;}
.CtxtMenu_MenuArrow {  position:absolute; right:.5em; padding-top:.25em; color:#666666;  font-family: null; font-size: .75em}
.CtxtMenu_MenuActive .CtxtMenu_MenuArrow {color:white}
.CtxtMenu_MenuArrow.CtxtMenu_RTL {left:.5em; right:auto}
.CtxtMenu_MenuCheck {  position:absolute; left:.7em;  font-family: null}
.CtxtMenu_MenuCheck.CtxtMenu_RTL { right:.7em; left:auto }
.CtxtMenu_MenuRadioCheck {  position:absolute; left: .7em;}
.CtxtMenu_MenuRadioCheck.CtxtMenu_RTL {  right: .7em; left:auto}
.CtxtMenu_MenuInputBox {  padding-left: 1em; right:.5em; color:#666666;  font-family: null;}
.CtxtMenu_MenuInputBox.CtxtMenu_RTL {  left: .1em;}
.CtxtMenu_MenuComboBox {  left:.1em; padding-bottom:.5em;}
.CtxtMenu_MenuSlider {  left: .1em;}
.CtxtMenu_SliderValue {  position:absolute; right:.1em; padding-top:.25em; color:#333333;  font-size: .75em}
.CtxtMenu_SliderBar {  outline: none; background: #d3d3d3}
.CtxtMenu_MenuLabel {  padding: 1px 2em 3px 1.33em;  font-style:italic}
.CtxtMenu_MenuRule {  border-top: 1px solid #DDDDDD;  margin: 4px 3px;}
.CtxtMenu_MenuDisabled {  color:GrayText}
.CtxtMenu_MenuActive {  background-color: #606872;  color: white;}
.CtxtMenu_MenuDisabled:focus {  background-color: #E8E8E8}
.CtxtMenu_MenuLabel:focus {  background-color: #E8E8E8}
.CtxtMenu_ContextMenu:focus {  outline:none}
.CtxtMenu_ContextMenu .CtxtMenu_MenuItem:focus {  outline:none}
.CtxtMenu_SelectionMenu {  position:relative; float:left;  border-bottom: none; -webkit-box-shadow:none; -webkit-border-radius:0px; }
.CtxtMenu_SelectionItem {  padding-right: 1em;}
.CtxtMenu_Selection {  right: 40%; width:50%; }
.CtxtMenu_SelectionBox {  padding: 0em; max-height:20em; max-width: none;  background-color:#FFFFFF;}
.CtxtMenu_SelectionDivider {  clear: both; border-top: 2px solid #000000;}
.CtxtMenu_Menu .CtxtMenu_MenuClose {  top:-10px; left:-10px}
</style><style id="MJX-CHTML-styles">
mjx-container[jax="CHTML"] {
  line-height: 0;
}

mjx-container [space="1"] {
  margin-left: .111em;
}

mjx-container [space="2"] {
  margin-left: .167em;
}

mjx-container [space="3"] {
  margin-left: .222em;
}

mjx-container [space="4"] {
  margin-left: .278em;
}

mjx-container [space="5"] {
  margin-left: .333em;
}

mjx-container [rspace="1"] {
  margin-right: .111em;
}

mjx-container [rspace="2"] {
  margin-right: .167em;
}

mjx-container [rspace="3"] {
  margin-right: .222em;
}

mjx-container [rspace="4"] {
  margin-right: .278em;
}

mjx-container [rspace="5"] {
  margin-right: .333em;
}

mjx-container [size="s"] {
  font-size: 70.7%;
}

mjx-container [size="ss"] {
  font-size: 50%;
}

mjx-container [size="Tn"] {
  font-size: 60%;
}

mjx-container [size="sm"] {
  font-size: 85%;
}

mjx-container [size="lg"] {
  font-size: 120%;
}

mjx-container [size="Lg"] {
  font-size: 144%;
}

mjx-container [size="LG"] {
  font-size: 173%;
}

mjx-container [size="hg"] {
  font-size: 207%;
}

mjx-container [size="HG"] {
  font-size: 249%;
}

mjx-container [width="full"] {
  width: 100%;
}

mjx-box {
  display: inline-block;
}

mjx-block {
  display: block;
}

mjx-itable {
  display: inline-table;
}

mjx-row {
  display: table-row;
}

mjx-row > * {
  display: table-cell;
}

mjx-mtext {
  display: inline-block;
}

mjx-mstyle {
  display: inline-block;
}

mjx-merror {
  display: inline-block;
  color: red;
  background-color: yellow;
}

mjx-mphantom {
  visibility: hidden;
}

_::-webkit-full-page-media, _:future, :root mjx-container {
  will-change: opacity;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-math {
  display: inline-block;
  text-align: left;
  line-height: 0;
  text-indent: 0;
  font-style: normal;
  font-weight: normal;
  font-size: 100%;
  font-size-adjust: none;
  letter-spacing: normal;
  border-collapse: collapse;
  word-wrap: normal;
  word-spacing: normal;
  white-space: nowrap;
  direction: ltr;
  padding: 1px 0;
}

mjx-container[jax="CHTML"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="CHTML"][display="true"][width="full"] {
  display: flex;
}

mjx-container[jax="CHTML"][display="true"] mjx-math {
  padding: 0;
}

mjx-container[jax="CHTML"][justify="left"] {
  text-align: left;
}

mjx-container[jax="CHTML"][justify="right"] {
  text-align: right;
}

mjx-mi {
  display: inline-block;
  text-align: left;
}

mjx-c {
  display: inline-block;
}

mjx-utext {
  display: inline-block;
  padding: .75em 0 .2em 0;
}

mjx-mo {
  display: inline-block;
  text-align: left;
}

mjx-stretchy-h {
  display: inline-table;
  width: 100%;
}

mjx-stretchy-h > * {
  display: table-cell;
  width: 0;
}

mjx-stretchy-h > * > mjx-c {
  display: inline-block;
  transform: scalex(1.0000001);
}

mjx-stretchy-h > * > mjx-c::before {
  display: inline-block;
  width: initial;
}

mjx-stretchy-h > mjx-ext {
  /* IE */ overflow: hidden;
  /* others */ overflow: clip visible;
  width: 100%;
}

mjx-stretchy-h > mjx-ext > mjx-c::before {
  transform: scalex(500);
}

mjx-stretchy-h > mjx-ext > mjx-c {
  width: 0;
}

mjx-stretchy-h > mjx-beg > mjx-c {
  margin-right: -.1em;
}

mjx-stretchy-h > mjx-end > mjx-c {
  margin-left: -.1em;
}

mjx-stretchy-v {
  display: inline-block;
}

mjx-stretchy-v > * {
  display: block;
}

mjx-stretchy-v > mjx-beg {
  height: 0;
}

mjx-stretchy-v > mjx-end > mjx-c {
  display: block;
}

mjx-stretchy-v > * > mjx-c {
  transform: scaley(1.0000001);
  transform-origin: left center;
  overflow: hidden;
}

mjx-stretchy-v > mjx-ext {
  display: block;
  height: 100%;
  box-sizing: border-box;
  border: 0px solid transparent;
  /* IE */ overflow: hidden;
  /* others */ overflow: visible clip;
}

mjx-stretchy-v > mjx-ext > mjx-c::before {
  width: initial;
  box-sizing: border-box;
}

mjx-stretchy-v > mjx-ext > mjx-c {
  transform: scaleY(500) translateY(.075em);
  overflow: visible;
}

mjx-mark {
  display: inline-block;
  height: 0px;
}

mjx-TeXAtom {
  display: inline-block;
  text-align: left;
}

mjx-mtable {
  display: inline-block;
  text-align: center;
  vertical-align: .25em;
  position: relative;
  box-sizing: border-box;
  border-spacing: 0;
  border-collapse: collapse;
}

mjx-mstyle[size="s"] mjx-mtable {
  vertical-align: .354em;
}

mjx-labels {
  position: absolute;
  left: 0;
  top: 0;
}

mjx-table {
  display: inline-block;
  vertical-align: -.5ex;
  box-sizing: border-box;
}

mjx-table > mjx-itable {
  vertical-align: middle;
  text-align: left;
  box-sizing: border-box;
}

mjx-labels > mjx-itable {
  position: absolute;
  top: 0;
}

mjx-mtable[justify="left"] {
  text-align: left;
}

mjx-mtable[justify="right"] {
  text-align: right;
}

mjx-mtable[justify="left"][side="left"] {
  padding-right: 0 ! important;
}

mjx-mtable[justify="left"][side="right"] {
  padding-left: 0 ! important;
}

mjx-mtable[justify="right"][side="left"] {
  padding-right: 0 ! important;
}

mjx-mtable[justify="right"][side="right"] {
  padding-left: 0 ! important;
}

mjx-mtable[align] {
  vertical-align: baseline;
}

mjx-mtable[align="top"] > mjx-table {
  vertical-align: top;
}

mjx-mtable[align="bottom"] > mjx-table {
  vertical-align: bottom;
}

mjx-mtable[side="right"] mjx-labels {
  min-width: 100%;
}

mjx-mtr {
  display: table-row;
  text-align: left;
}

mjx-mtr[rowalign="top"] > mjx-mtd {
  vertical-align: top;
}

mjx-mtr[rowalign="center"] > mjx-mtd {
  vertical-align: middle;
}

mjx-mtr[rowalign="bottom"] > mjx-mtd {
  vertical-align: bottom;
}

mjx-mtr[rowalign="baseline"] > mjx-mtd {
  vertical-align: baseline;
}

mjx-mtr[rowalign="axis"] > mjx-mtd {
  vertical-align: .25em;
}

mjx-mtd {
  display: table-cell;
  text-align: center;
  padding: .215em .4em;
}

mjx-mtd:first-child {
  padding-left: 0;
}

mjx-mtd:last-child {
  padding-right: 0;
}

mjx-mtable > * > mjx-itable > *:first-child > mjx-mtd {
  padding-top: 0;
}

mjx-mtable > * > mjx-itable > *:last-child > mjx-mtd {
  padding-bottom: 0;
}

mjx-tstrut {
  display: inline-block;
  height: 1em;
  vertical-align: -.25em;
}

mjx-labels[align="left"] > mjx-mtr > mjx-mtd {
  text-align: left;
}

mjx-labels[align="right"] > mjx-mtr > mjx-mtd {
  text-align: right;
}

mjx-mtd[extra] {
  padding: 0;
}

mjx-mtd[rowalign="top"] {
  vertical-align: top;
}

mjx-mtd[rowalign="center"] {
  vertical-align: middle;
}

mjx-mtd[rowalign="bottom"] {
  vertical-align: bottom;
}

mjx-mtd[rowalign="baseline"] {
  vertical-align: baseline;
}

mjx-mtd[rowalign="axis"] {
  vertical-align: .25em;
}

mjx-msup {
  display: inline-block;
  text-align: left;
}

mjx-c::before {
  display: block;
  width: 0;
}

.MJX-TEX {
  font-family: MJXZERO, MJXTEX;
}

.TEX-B {
  font-family: MJXZERO, MJXTEX-B;
}

.TEX-I {
  font-family: MJXZERO, MJXTEX-I;
}

.TEX-MI {
  font-family: MJXZERO, MJXTEX-MI;
}

.TEX-BI {
  font-family: MJXZERO, MJXTEX-BI;
}

.TEX-S1 {
  font-family: MJXZERO, MJXTEX-S1;
}

.TEX-S2 {
  font-family: MJXZERO, MJXTEX-S2;
}

.TEX-S3 {
  font-family: MJXZERO, MJXTEX-S3;
}

.TEX-S4 {
  font-family: MJXZERO, MJXTEX-S4;
}

.TEX-A {
  font-family: MJXZERO, MJXTEX-A;
}

.TEX-C {
  font-family: MJXZERO, MJXTEX-C;
}

.TEX-CB {
  font-family: MJXZERO, MJXTEX-CB;
}

.TEX-FR {
  font-family: MJXZERO, MJXTEX-FR;
}

.TEX-FRB {
  font-family: MJXZERO, MJXTEX-FRB;
}

.TEX-SS {
  font-family: MJXZERO, MJXTEX-SS;
}

.TEX-SSB {
  font-family: MJXZERO, MJXTEX-SSB;
}

.TEX-SSI {
  font-family: MJXZERO, MJXTEX-SSI;
}

.TEX-SC {
  font-family: MJXZERO, MJXTEX-SC;
}

.TEX-T {
  font-family: MJXZERO, MJXTEX-T;
}

.TEX-V {
  font-family: MJXZERO, MJXTEX-V;
}

.TEX-VB {
  font-family: MJXZERO, MJXTEX-VB;
}

mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c {
  font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A ! important;
}

@font-face /* 0 */ {
  font-family: MJXZERO;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Zero.woff") format("woff");
}

@font-face /* 1 */ {
  font-family: MJXTEX;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff") format("woff");
}

@font-face /* 2 */ {
  font-family: MJXTEX-B;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff") format("woff");
}

@font-face /* 3 */ {
  font-family: MJXTEX-I;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff") format("woff");
}

@font-face /* 4 */ {
  font-family: MJXTEX-MI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff") format("woff");
}

@font-face /* 5 */ {
  font-family: MJXTEX-BI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff") format("woff");
}

@font-face /* 6 */ {
  font-family: MJXTEX-S1;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff") format("woff");
}

@font-face /* 7 */ {
  font-family: MJXTEX-S2;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff") format("woff");
}

@font-face /* 8 */ {
  font-family: MJXTEX-S3;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff") format("woff");
}

@font-face /* 9 */ {
  font-family: MJXTEX-S4;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff") format("woff");
}

@font-face /* 10 */ {
  font-family: MJXTEX-A;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff") format("woff");
}

@font-face /* 11 */ {
  font-family: MJXTEX-C;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff") format("woff");
}

@font-face /* 12 */ {
  font-family: MJXTEX-CB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff") format("woff");
}

@font-face /* 13 */ {
  font-family: MJXTEX-FR;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff") format("woff");
}

@font-face /* 14 */ {
  font-family: MJXTEX-FRB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff") format("woff");
}

@font-face /* 15 */ {
  font-family: MJXTEX-SS;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff") format("woff");
}

@font-face /* 16 */ {
  font-family: MJXTEX-SSB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff") format("woff");
}

@font-face /* 17 */ {
  font-family: MJXTEX-SSI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff") format("woff");
}

@font-face /* 18 */ {
  font-family: MJXTEX-SC;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff") format("woff");
}

@font-face /* 19 */ {
  font-family: MJXTEX-T;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff") format("woff");
}

@font-face /* 20 */ {
  font-family: MJXTEX-V;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff") format("woff");
}

@font-face /* 21 */ {
  font-family: MJXTEX-VB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff") format("woff");
}

mjx-c.mjx-c1D719.TEX-I::before {
  padding: 0.694em 0.596em 0.205em 0;
  content: "\3D5";
}

mjx-c.mjx-c3A::before {
  padding: 0.43em 0.278em 0 0;
  content: ":";
}

mjx-c.mjx-c53.TEX-C::before {
  padding: 0.705em 0.642em 0.022em 0;
  content: "S";
}

mjx-c.mjx-c2192::before {
  padding: 0.511em 1em 0.011em 0;
  content: "\2192";
}

mjx-c.mjx-c5A.TEX-C::before {
  padding: 0.683em 0.767em 0 0;
  content: "Z";
}

mjx-c.mjx-c1D451.TEX-I::before {
  padding: 0.694em 0.52em 0.01em 0;
  content: "d";
}

mjx-c.mjx-c2217::before {
  padding: 0.465em 0.5em 0 0;
  content: "\2217";
}

mjx-c.mjx-c28::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: "(";
}

mjx-c.mjx-c1D460.TEX-I::before {
  padding: 0.442em 0.469em 0.01em 0;
  content: "s";
}

mjx-c.mjx-c2C::before {
  padding: 0.121em 0.278em 0.194em 0;
  content: ",";
}

mjx-c.mjx-c1D454.TEX-I::before {
  padding: 0.442em 0.477em 0.205em 0;
  content: "g";
}

mjx-c.mjx-c29::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: ")";
}

mjx-c.mjx-c3D::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "=";
}

mjx-c.mjx-c2016::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "\2225";
}

mjx-c.mjx-c2212::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "\2212";
}

mjx-c.mjx-c2208::before {
  padding: 0.54em 0.667em 0.04em 0;
  content: "\2208";
}

mjx-c.mjx-c1D70B.TEX-I::before {
  padding: 0.431em 0.57em 0.011em 0;
  content: "\3C0";
}

mjx-c.mjx-c1D44E.TEX-I::before {
  padding: 0.441em 0.529em 0.01em 0;
  content: "a";
}

mjx-c.mjx-c2223::before {
  padding: 0.75em 0.278em 0.249em 0;
  content: "\2223";
}

mjx-c.mjx-c1D467.TEX-I::before {
  padding: 0.442em 0.465em 0.011em 0;
  content: "z";
}

mjx-c.mjx-c1D45F.TEX-I::before {
  padding: 0.442em 0.451em 0.011em 0;
  content: "r";
}

mjx-c.mjx-c2032::before {
  padding: 0.56em 0.275em 0 0;
  content: "\2032";
}

mjx-c.mjx-c27E8::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: "\27E8";
}

mjx-c.mjx-c27E9::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: "\27E9";
}

mjx-c.mjx-c2E::before {
  padding: 0.12em 0.278em 0 0;
  content: ".";
}
</style></head>

<body data-new-gr-c-s-check-loaded="14.1220.0" data-gr-ext-installed="">
<div class="container" id="main">
    <div class="row">
        <h1 class="col-md-12 text-center text-title top20">
            <b>
                Why RL Is Still Stuck?
            </b><br>
            <small>
                <b>Research statement</b>
            </small>
        </h1>
    </div>

    <div class="row">
        <div class="col-md-12 text-center">
            <ul class="list-inline">
                <li>
                    <a href="https://dongsuleetech.github.io/">Dongsu Lee</a><br>UT Austin<br> August 2025
                </li>
            </ul>
        </div>
    </div>

    <div class="row top15 bottom30">
        <div class="col-md-12 offset-md-2">
<!--            <h2>-->
<!--                What Does It Take to Accelerate MARL?</h2>-->
            <p>
            <strong>TL;DR</strong>: Machine learning crushed it by aligning data, compute, and training paradigms.
                Control, especially in reinforcement learning (RL), hasn’t had that “GPT moment” because the problem is not trivial like sing-agent assumption.
                If we want AI that blends into complex society, we need multi-agent RL (MARL) that generalizes:
                (1) capture joint distributions,
                (2) learn the representations in multi-agent systems,
                (3) achieve rapid compatibility.
                Below is a step-by-step roadmap, and a concrete call for research.
            </p>
        </div>
    </div>
    <div class="row top15 bottom30">
        <div class="col-md-12 offset-md-2">
            <h2>
                The contrast: Why control lags where LLMs soared </h2>
            <p>
                For decades, machine learning has been absolutely crushing it
                whenever massive datasets, scalable compute, novel training paradigms,
                and advanced architectures come together. Just look at large language models,
                such as <a href="https://chatgpt.com/">GPT</a>, <a href="https://gemini.google.com/app">Gemini</a>,
                and <a href="https://grok.com/">Grok</a>; they’ve completely redefined how we interact with AI,
                from casual conversation to serious problem-solving. On the vision side,
                <a href="https://arxiv.org/pdf/2403.09611?">multi-modal</a>
                <a href="https://llava-vl.github.io/">models</a>
                are pushing boundaries in image understanding and creative generation.
            </p>
            <p>
                <i>But what about the control domain?</i>
            </p>
            <p>
               Here the story changes. Despite years of research,
                learning-based control hasn't had its "GPT moment." More recently, Sergey Levine highlights
                in <a href="https://sergeylevine.substack.com/p/the-promise-of-generalist-robotic">The Promise of Generalist Robotic Policies</a>
                that scaling to general-purpose control will require not only larger models, but also diverse,
                cross-embodiment data and foundation-style training.
                Following this instruction, we’ve seen exciting releases like
                <a href="https://robotics-transformer-x.github.io/">RT-X</a> and
                <a href="https://www.physicalintelligence.company/blog/pi0">\(\pi_0\)</a>, but so far
                there’s no breakthrough that’s deeply embedded in everyday practice.
            </p>
            <p>
                As several recent columns point out, this isn’t surprising.
                <a href="https://ysymyth.github.io/The-Second-Half/">Second Half of AI</a> argues
                that RL has long been constrained by benchmark-centric progress and limited
                transfer to real-world impact. Silver and Sutton’s
                <a href="https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf">
                    Welcome to the Era of Experience</a> similarly highlights that the next leap will come
                not from bigger datasets alone, but from agents that learn through their own interactions and experiences.
                These perspectives capture precisely why learning-based control has lagged:
                scaling model size or algorithmic tweaks isn’t enough
                when the problem itself is about acting and adapting in complex, open environments.
            </p>
            <p>
                I believe AI agents cannot be designed as isolated problem-solvers
                built on strong assumptions if they are ever to integrate seamlessly into society.
                The real world is inherently multi-agent system: environments are only partially observable,
                agents constantly encounter diverse and unpredictable counterparts,
                and the underlying dynamics are often ambiguous and shifting. Therefore, progress
                in control won’t just come from scaling single-agent learning,
                but from developing methods that let agents adapt, interact,
                and evolve within the complex fabric of human society.
            </p>
        </div>
    </div>
    <div class="row top15 bottom30">
        <div class="col-md-12 offset-md-2">
            <h2>
                The core bottlenecks of RL towards multi-agent systems </h2>
            <p>
                Meanwhile, generalizability has been a key challenge in RL
                and its promising workarounds are self-play and representation learning,
                achieving notable success in scalable problems.
                We have already seen its power in the single-agent setting.
                For example, landmark achievements like
                <a href="https://www.nature.com/articles/nature16961.epdf?author_access_token=P3HwdfhgethMkcPAapNCwdRgN0jAjWel9jnR3ZoTv0OivKk3lXs6SxMz535byYwHuxqGq5vmEcS5N5QBJ1slzccjly0jMlAcdnFE0xdmBJ2_dzS9DvjIyJuCKT5mGOqL">
                    AlphaGo</a>,
                <a href="https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/"> AlphaZero</a>,
                and <a href="https://deepmind.google/discover/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules/">MuZero</a>
                reached impressive generalization across games largely thanks to on-policy self-play,
                which explored and exploited complete information environments to policies.
                A longside this, awave of research on representation for RL, <i>e.g.</i>,
                <a href="https://arxiv.org/abs/1606.05312">successor</a> <a href="https://arxiv.org/abs/2411.19418">features</a>,
                <a href="https://arxiv.org/abs/2103.07945">forward-backward</a> <a href="https://kvfrans.com/successor-representations-explained/">representations</a>,
                <a href="https://arxiv.org/abs/2006.10742">bisimulation</a>,
                <a href="https://arxiv.org/abs/2304.01203">Quasimetric</a>,
                <a href="https://arxiv.org/abs/2402.15567">Hilbert</a>,
                <a href="https://arxiv.org/abs/2505.13144">temporal distance</a>,
                and <a href="https://arxiv.org/pdf/2206.07568">contrastive learning</a>.
                These approaches showed that better abstractions of the MDP could improve generalization and transfer,
                turning narrow policies into more versatile ones.
            </p>
            <p>
                Unfortunately, unlike single-agent RL, where
                <i>(i)</i> the environment interact with a single agent and,
                <i>(ii)</i> representations can be derived from fully observable dynamics,
                extending this perspective to multi-agent systems is far from trivial.
                That is because MARL requires
                considering the more exploration and interactions as increase the number of agents, and
                handling diverse information such as inter-agent relationships, their role structures,
                and shared world dynamics under a partially observable MDP.
                While some methods leverage one of these aspects in isolation,
                there has been limited progress in integrating them into a unified representation
                that captures the factors facilitating team coordination.
                From this perspective, I envision several fruitful directions for future works as Call-for-Research:
            </p>
        </div>
    </div>

<!--    <div class="row top15 bottom30">-->
<!--        <div class="col-md-12 offset-md-2">-->
<!--            <h2>Why Learning-based Control?</h2>-->
<!--            <p>-->
<!--                For decades,-->
<!--                machine learning has been killing it in areas like computer vision.-->
<!--                But what about control?-->
<!--                Y'all know, making machines not just see but act smartly in real-time?-->
<!--                That’s where my curiosity kicked in.-->
<!--                I’m hooked on exploring how learning-based control can make an AI agent smarter and more adaptive.-->
<!--            </p>-->
<!--        </div>-->
<!--    </div>-->
<!--    <div class="row">-->
<!--        <div class="col-md-12 offset-md-2">-->
<!--            <h2>Why Multi-agent System?</h2>-->
<!--            <p>-->
<!--                Well, look around the world is full of them!-->
<!--                From flocks of birds to rush-hour traffic to teams of surgical robots, everything involves agents interacting and collaborating.-->
<!--                By teaching AI to handle these complex group dynamics,-->
<!--                we can unlock next-level problem-solving.-->
<!--                The potential to boost human well-being through Human-AI and AI-AI teamwork?-->
<!--                Definitely, I believe it’s huge.-->
<!--            </p>-->
<!--        </div>-->
<!--    </div>-->
<!--    <div class="row">-->
<!--        <div class="col-md-12 offset-md-2">-->
<!--            <h2>Do Prior MARLs Scale to Real-World Problems?</h2>-->
<!--            <p>-->
<!--               Let’s take a step back and ask: <b>How have MARL algorithms been improving over time—and are they headed in the right direction?</b>-->
<!--            </p>-->
<!--            <p>-->
<!--                A lot of current MARL research started by extending single-agent methods like PPO, TD3, and Q-learning.-->
<!--                And sure, these algorithms perform impressively in clean simulation setups. But here’s the catch: real-world environments are messy.-->
<!--                They're full of partial observability, dynamic teammate changes, non-stationarity, and unclear credit assignment.-->
<!--                Just plugging a single-agent solution into a multi-agent setting often doesn’t cut it.-->
<!--            </p>-->
<!--            <p>-->
<!--                Some techniques like value decomposition or policy factorization have tried to bridge this gap.-->
<!--                They help, but only up to a point.-->
<!--                Most existing successes are still confined to relatively small-scale setups—simple grid worlds, a handful of agents, and fully discrete action spaces.-->
<!--                So the big question is: <b>Why can’t we go beyond toy environments?</b>-->
<!--            </p>-->
<!--        </div>-->
<!--    </div>-->
<!--    <div class="row">-->
<!--        <div class="col-md-12 offset-md-2">-->
<!--        <h2>Why Are Prior MARLs Stuck at the GUI-level Environment? (ongoing)</h2>-->
<!--            <p>-->
<!--               We’ve seen breakthroughs in language models, vision, and even single-agent RL. So why is multi-agent RL still stuck playing capture-the-flag in a 2D grid?-->
<!--                </p>-->
<!--            <p>-->
<!--                The truth is: most MARL methods just don’t scale well when the environment becomes too complex, the horizon too long, or the teammates too unpredictable.</p>-->
<!--        </div>-->
<!--    </div>-->

<!--    <div class="row">-->
<!--        <div class="col-md-12 offset-md-2">-->
<!--            <h2>Research Statement</h2>-->
<!--                <h3>Test-time coordination with unseen agents:<br>-->
<!--                    <em>Can RL agents effectively coordinate with partners they've never encountered before?</em> </h3>-->
<!--                <p> Real-world applications frequently require agents to cooperate instantly with unfamiliar teammates.-->
<!--                    This coordination is challenging, especially when agents operate under partial observability and uncertainty about others' intentions.-->
<!--                    Inspired by human cognition, I enable RL agents to predict future actions of unseen teammates-->
<!--                    by inferring their internal preferences from their observed behaviors (<a href="https://dongsuleetech.github.io/projects/eftm/">NeurIPS 2024</a>).-->
<!--                    Recently, I have focused more on developing the generalization for zero-shot coordination. <br>-->
<!--                    </p>-->
<!--                <p> Open challenge is: How can agents scale this inference capability efficiently to very large teams or continuously evolving environments? </p>-->

<!--                <h3> Communication multi-agent RL:<br>-->
<!--                    <em>How can agents communicate efficiently under real-world constraints such as limited bandwidth or noisy environments?</em></h3>-->
<!--                <p>-->
<!--                    For teams of robots or virtual agents to succeed, communication is essential—yet it comes at a cost in the real world.-->
<!--                    Bandwidth constraints, noisy environments, and adversarial disruptions demand smarter, more efficient communication protocols.-->
<!--                    Ideally, agents should adaptively choose what information to share, with whom, and when, while minimizing the cost and complexity of messages.-->
<!--                    </p>-->
<!--                <p>-->
<!--                    To address these challenges, I have developed methods that optimize communication among agents via-->
<!--                    (1) vector quantization and (2) implicit communication (under submission).-->
<!--                    Through these methods, we allow agents to decide communication links and encode information under realistic limitations;-->
<!--                    allow coordination without explicit communication at deployment.-->
<!--                    </p>-->
<!--                <p> Open Challenge is: How can we further refine the communication protocol to ensure reliable interaction between AI agents and humans,-->
<!--                    particularly when they have limited prior knowledge of each other's goals or capabilities? </p>-->

<!--                <h3> Efficient representation for RL:<br>-->
<!--                    <em>What is the most efficient way to represent complex environments to boost the effectiveness of RL algorithms?</em>-->
<!--                    </h3>-->
<!--                <p> RL agents need effective internal representations of their complex surroundings to make decisions efficiently.-->
<!--                    The right representation makes learning faster, generalization easier, and coordination smoother,-->
<!--                    especially in multi-agent settings or challenging environments with sparse rewards and long horizons.-->
<!--                    </p>-->
<!--                <p> My work introduced new methods for learning these compact yet powerful representations.-->
<!--                    Specifically, TempDATA utilizes temporal distance-aware abstractions, enhancing agents' abilities to generalize across complex,-->
<!--                    long-horizon tasks, significantly improving data efficiency in offline RL settings (<a href="https://dongsuleetech.github.io/projects/tempdata/">ICML 2025</a>).-->
<!--                    Additionally, my research on representation for MARL demonstrated that latent representations capturing inter-agent relationships-->
<!--                    and task-specific global states substantially improve multi-agent coordination.-->
<!--                    </p>-->
<!--                <p> Open challenge is: How do we automatically discover representations that not only improve efficiency-->
<!--                    but also generalize robustly across diverse, unseen environments and tasks?-->
<!--                    </p>-->
<!--        </div>-->
<!--    </div>-->

    <script>
        if (navigator.userAgent.match(/Android/i)
            || navigator.userAgent.match(/webOS/i)
            || navigator.userAgent.match(/iPhone/i)
            || navigator.userAgent.match(/iPad/i)
            || (navigator.userAgent.includes("Mac") && "ontouchend" in document)
            || navigator.userAgent.match(/iPod/i)
            || navigator.userAgent.match(/BlackBerry/i)
            || navigator.userAgent.match(/Windows Phone/i)) {
            const cells = document.getElementsByTagName('video');
            for (const cell of cells) {
                cell.removeAttribute("controls");
            }
        }
    </script>
</div>


</body><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select:none;
        user-select:none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>